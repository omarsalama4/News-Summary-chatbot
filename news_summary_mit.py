# -*- coding: utf-8 -*-
"""News_Summary_MIT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I966e_sspIVD7XkEPcjKCktzWm8wdobX

# MIT News Summary with Gemini API
This notebook demonstrates how to summarize news articles using the Gemini API.
It includes functionality to summarize articles, extract topics, and generate questions based on the content.
"""

# 1.Upload Data
"""

!pip install datasets

from datasets import load_dataset

# Load CNN/DailyMail summarization dataset
dataset = load_dataset("cnn_dailymail", "3.0.0")

# View a sample
print(dataset['train'][0])

dataset

import pandas as pd

train_dataset = dataset['train']

df_train = dataset['train'].to_pandas()

df_train['id'].nunique()

df_train['article'][0]

df_train['highlights'][0]

"""# 2.Integrate GMININI Model"""

!pip install -q -U google-genai

import google.generativeai as genai

# Set your preferred model
model_name = "gemini-2.5-flash"

# Configure Gemini
genai.configure(api_key="AIzaSyAnkTrK9iCYQPzG9lIaBrhKU-as8PWqHZY")

# Initialize the model directly
model = genai.GenerativeModel(model_name)

"""#3.Prompting"""

!pip install rouge_score
import time
import json
import pandas as pd
from tqdm import tqdm
from rouge_score import rouge_scorer
from concurrent.futures import ThreadPoolExecutor

def calculate_rouge(summary, reference):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    return scorer.score(reference, summary)

def safe_generate(prompt, retries=3, delay=2):
    for attempt in range(retries):
        try:
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            print(f"Retrying after error: {e}")
            time.sleep(delay)
    return "Error: Failed after retries"

def truncate_text(text, max_words=400):
    return ' '.join(text.split()[:max_words])

def summarize_article(article):
    article_id = article.get("id")
    article_text = truncate_text(article["article"])
    highlight = article.get("highlights", "")

    try:
        summary_prompt = f"Summarize the following news article in no more than 5 concise, factual sentences:\n\n{article_text}"
        summary_response = safe_generate(summary_prompt)

        topics_prompt = f"Read the following article and list the top 3 topics it covers, in bullet points:\n\n{article_text}"
        topics_response = safe_generate(topics_prompt)

        questions_prompt = f"Based on the article below, write 3 thought-provoking questions a reader might ask after reading it:\n\n{article_text}"
        questions_response = safe_generate(questions_prompt)

        # Evaluate ROUGE between summary and highlight
        rouge_score = calculate_rouge(summary_response, highlight)

        return {
            "id": article_id,
            "summary": summary_response,
            "topics": topics_response,
            "questions": questions_response,
            "highlight": highlight,
            "rouge1": rouge_score["rouge1"].fmeasure,
            "rouge2": rouge_score["rouge2"].fmeasure,
            "rougeL": rouge_score["rougeL"].fmeasure
        }

    except Exception as e:
        return {
            "id": article_id,
            "summary": f"Error: {e}",
            "topics": "Error",
            "questions": "Error",
            "highlight": highlight,
            "rouge1": 0.0,
            "rouge2": 0.0,
            "rougeL": 0.0
        }

def process_all_parallel(data_dict, batch_size=8):
    data = [
        {
            'id': data_dict['id'][i],
            'article': data_dict['article'][i],
            'highlights': data_dict['highlights'][i]
        }
        for i in range(len(data_dict['id']))
    ]

    all_results = []
    failed = []

    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]
        with ThreadPoolExecutor() as executor:
            results = list(executor.map(summarize_article, batch))

        for res in results:
            if "Error" in res["summary"]:
                failed.append(res["id"])
            all_results.append(res)

    return all_results, failed

def process_all_sequential(data_dict):
    data = [
        {
            'id': data_dict['id'][i],
            'article': data_dict['article'][i],
            'highlights': data_dict['highlights'][i]
        }
        for i in range(len(data_dict['id']))
    ]

    all_results = []
    for entry in tqdm(data):
        result = summarize_article(entry)
        all_results.append(result)

    return all_results

"""#4.Evaluate"""

def save_results(results, json_path="output.json", csv_path="output.csv"):
    with open(json_path, "w") as f:
        json.dump(results, f, indent=2)
    pd.DataFrame(results).to_csv(csv_path, index=False)

results, failed_ids = process_all_parallel(train_dataset.select(range(3)))
save_results(results)

print("Done âœ…")
print(f"Failed article IDs: {failed_ids}")

display(results)

search_id = results[0]['id']

article_by_id = train_dataset.filter(lambda example: example['id'] == search_id)

# Display the result
if len(article_by_id) > 0:
    display(article_by_id[0])
else:
    print(f"Article with ID '{search_id}' not found in the train dataset.")

